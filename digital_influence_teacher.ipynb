{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Data Collection and Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook:\n",
    "\n",
    "1. Attaching to the Twitter API\n",
    "2. Searching for a specific user\n",
    "3. Searching for a specific topic\n",
    "4. Extending the search and working with multi-level JSON Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attaching to the Twitter API\n",
    "\n",
    "## Questions & Objectives\n",
    "\n",
    "* Setting up access and validity signing\n",
    "* Setting up a handler to manage the connection\n",
    "* Running a test search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we will download the libraries that deal with accessing the API (tweepy) and working with the JSON data (json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell now to import the libraries\n",
    "!pip install tweepy\n",
    "import tweepy #https://github.com/tweepy/tweepy\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then set up the variables that hold the validation keys. You need to add your keys (tokens) and secrets in the spaces below. Make sure to put them between the speech marks and make sure there is not extra spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in your keys and secrets then run this cell\n",
    "\n",
    "api_key = 'VI0ZZrBl2EAqOnr6aDpYIUGZv'\n",
    "api_secret = 'RuCXZuvQyNMzOeH6ud7tCUluTSpQlwRA56H2P197tkC6QbmaEP'\n",
    "access_key = '1396862365421932546-uPZLe3gOmjKAMekmptanUfXeXihtIl'\n",
    "access_secret = 'rAWcjW3AKGbkqKXV83cqXI1XSQFb2dd6M690ZWBWIhzQs'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then set up the authication handler. We pass the keys and secrets as below and then set up the api object. We can then use this object to attach to the API each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(api_key, api_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the connection we will run a test query.\n",
    "\n",
    "We use the API object and we are going to ask for some of the tweets from users that you follow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_tweets = api.home_timeline()\n",
    "for tweet in public_tweets:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching for a Specific User\n",
    "\n",
    "* Search for a specific user\n",
    "* Retrieve data from the Twitter API\n",
    "* Call specific items from the JSON data object\n",
    "* Look at the full JSON data\n",
    "\n",
    "We will now look for tweets from a specific person. To do this we need their Twitter name. If you go to https://twitter.com/NicolaSturgeon you can see the twitter name under the main name. You can see it has a @ sign in front that we remove.  \n",
    "\n",
    "For this we use the **get_user** method from the Twitter API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create an object and call the information on the user Nicola Sturgeon and hold it in the object.\n",
    "\n",
    "user = api.get_user(screen_name='NicolaSturgeon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This object is in JSON tuples.\n",
    "# We can call the tuples and print their content. \n",
    "# we will look more at JSON later\n",
    "# We can print the screen name as below\n",
    "\n",
    "print(user.screen_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can print the number of followers -- check this is correct on the link to the Twitter page\n",
    "\n",
    "print(user.followers_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We Can print the user description \n",
    "\n",
    "print(user.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see all of the user information in it's raw format we can type:\n",
    "\n",
    "print(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minitask - For you to work on when you try\n",
    "\n",
    "* Try using the information from the user print out to access the other information.\n",
    "* See if you can work out how to get to the nested tuples\n",
    "* Try and look at another user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get tweets from the API user timeline\n",
    "# This time we call the user_timeline method again with the NicolaSturgeon user method\n",
    "# Here we call the last two tweets\n",
    "# These are retured in a list object\n",
    "\n",
    "new_tweets = api.user_timeline(screen_name = 'NicolaSturgeon',count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can tweet the first tweet (which remember is 0 in a list)\n",
    "# What other information can you access from the tweets?  How about the number of retweets?\n",
    "new_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can tweet the first tweet (which remember is 0 in a list)\n",
    "# What other information can you access from the tweets?  How about the number of retweets?\n",
    "new_tweets[0].text\n",
    "#new_tweets[0].created_at"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching for a Topic\n",
    "\n",
    "* Search the twitter API using a key word\n",
    "* Retrieve the text from a single tweets\n",
    "* Retrieve the text from multiple tweets\n",
    "* Process and clean the text\n",
    "* Visualise the text\n",
    "\n",
    "We will now look for tweets that contain a specific word. \n",
    "\n",
    "For this we use the **search** method from the Twitter API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are looking for the word covid\n",
    "# We are asking for 10 english tweets to be returned\n",
    "# This is returned as a list\n",
    "\n",
    "covid_tweets = api.search_tweets(q='covid', lang='en', count='10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can print out the first in the list \n",
    "\n",
    "covid_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we can't just call the JSON from the object (like we did with the user object)\n",
    "We have to deal with the JSON directly. We do this using the json function\n",
    "Then we can call all of the tuples as a dictionary object. \n",
    "\n",
    "(remember a tuple take the form ['text':'this is tweet text'] this means that we can call for the content of the tuple by the key of the tuple.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we can see all of the json in a nice format\n",
    "\n",
    "covid_tweets[0]._json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or we can just call the text\n",
    "\n",
    "covid_tweets[0]._json['text'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can text put the text into it's own list and just work with just the text\n",
    "\n",
    "tweets_text = []\n",
    "for each in covid_tweets:\n",
    "    tweets_text.append(each._json['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see how we have put this into a list\n",
    "\n",
    "print(tweets_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example we can turn it into a string and tokenise it\n",
    "\n",
    "tweets_string = \" \".join(tweets_text)\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(tweets_string)\n",
    "print(tokens[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can clean it up, making it all lowercase and removing stopwords\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "lowercase_tokens = [token.lower() for token in tokens]\n",
    "remove_these = set(stopwords.words('english') + list(string.punctuation) + list(string.digits))\n",
    "filtered_text = [word \n",
    "                 for word in lowercase_tokens \n",
    "                 if not word in remove_these]\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can produce word frequencies\n",
    "from collections import Counter\n",
    "simple_frequencies_dict = Counter(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And word clouds\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "cloud = WordCloud(max_font_size=80,colormap=\"hsv\").generate_from_frequencies(simple_frequencies_dict)\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.imshow(cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minitask - For you to work on when you try\n",
    "\n",
    "* Try searching for a differnt word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending the search and working with multi-level JSON Data\n",
    "\n",
    "* Search the Twitter API using an extended query with multiple terms\n",
    "* Search using a tweepy cursor to retrieve more data\n",
    "* Look at nested data from the JSON\n",
    "\n",
    "We will now look for tweets that contain several words. We can combine query words with the operator 'OR'. This operator say give me tweets that contain word1 or word2. You might want to do this with related words on the same topic or use it to cover multiple spellings or typos. \n",
    "\n",
    "For this we use the **search** method from the Twitter API.\n",
    "\n",
    "We want to gather more data than we did before. The search method limits the data we can retrieve. To extend the amount of data we retrieve we use a Tweepy Cursor. Twitter returns multiple pages of data. Almost like a book, but it will only give you one page at a time. Before we only took the first page. This time we will page through the extended version using a cursor object. The cursor maintain the connection with the API and allows us to ask for the next page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set up a list to hold the tweets so we can then append to it as we iterate through the pages.\n",
    "# Previously, we created a list in the search, but here we need to create a list so we can add to it.\n",
    "\n",
    "covid_tweets = []\n",
    "\n",
    "# We set up a tweepy Cursor to maintain the connection.\n",
    "# We set up the query with the OR operator.\n",
    "# We iterate through the pages from the API using a for loop.\n",
    "# We append the content to a list.\n",
    "for page in tweepy.Cursor(api.search_tweets, \n",
    "                          q='covid OR covid19 OR COVID OR COVID19 or #covid', \n",
    "                          lang='en').pages(10):\n",
    "    covid_tweets.append(page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see the text from the first tweet\n",
    "\n",
    "print(covid_tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see the text from the first tweet:\n",
    "\n",
    "print(covid_tweets[0][0].text) # covid_tweets[0][0] is the first Status (tweet) object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter data is nested.\n",
    "\n",
    "This means that it can contain items within items.\n",
    "\n",
    "For example hashtags, user mentions, and URLs are contained within an entities dictionary.\n",
    "\n",
    "This looks like:\n",
    "\n",
    "'entities': { \n",
    "    'hashtags': [{'hashtag1'}, {'hashtag2'}], \n",
    "    'user_mentions': [{'screen_name':'barackobama', 'name': 'Barack Obama'}], \n",
    "    'urls': [{'url':'www.bbc.co.uk'}]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hashtags are contained in a list within the entity tuple,\n",
    "# which means we need to call the entity tuple (hashtag) and then iterate through the list.\n",
    "# We set up a list to hold the hashtags so we can then append to it as we iterate.\n",
    "# We iterate through each tweet, and then through the hashtags in the list,\n",
    "# adding the tweets to the list.\n",
    "\n",
    "covid_hashtags = []\n",
    "for search_result in covid_tweets:\n",
    "    for status in search_result:  # for every tweet\n",
    "        hashtags = status.entities['hashtags']\n",
    "        if len(hashtags) > 0:     # if there are hashtags\n",
    "            for h in hashtags:\n",
    "                covid_hashtags.append(h['text'])\n",
    "\n",
    "print(covid_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can then visualise these hashtags in the ways we learnt before.\n",
    "\n",
    "hashtag_string = \" \".join(covid_hashtags)\n",
    "tokens = word_tokenize(hashtag_string)\n",
    "simple_frequencies_dict_covid = Counter(tokens)\n",
    "cloud = WordCloud(width=800, height=400, max_font_size=160, \n",
    "                  colormap=\"viridis\", \n",
    "                  background_color='white',).generate_from_frequencies(simple_frequencies_dict_covid)\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.imshow(cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minitask - for you to try when you work through\n",
    "\n",
    "* Try using the creating a visualisation with a different nested item"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
